# Pre-requisites:
# - AWS Account with correct permissions
# - `gh-pages` branch
# - AWS Account needs to add the LOG_GROUP_NAME defined below

name: Soak Testing
on:
  schedule:
    - cron: '0 */24 * * *'
env:
  # Language Specific
  PROCESS_COMMAND_LINE_DIMENSION_VALUE: /usr/local/bin/python3 application.py
  PROCESS_EXECUTABLE_NAME: python3
  # - Every language must also set the path for the sample-app in the `Build and Push Docker image` step.
  #   For example, Python is using this path for it's sample-app context:
  #   `context: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}`

  # Additional Env Vars
  AWS_DEFAULT_REGION: us-east-1
  TEST_DURATION_MINUTES: 300
  HOSTMETRICS_INTERVAL_SECS: 600
  CPU_LOAD_THRESHOLD: 75
  TOTAL_MEMORY_THRESHOLD: 1610612736
  # TODO: We might be able to adapt the "Soak Tests" to be "Overhead Tests". This means monitoring the Sample App's
  # performance using high levels of TPS for the Load Generator over a shorter period of testing time.
  # For example: https://github.com/aws-observability/aws-otel-collector/blob/main/docs/performance_model.md
  # THROUGHPUT_PER_SECOND: TBD?

jobs:
  test_apps_and_publish_results:
    name: Publish app and Soak Performance Test - (${{ matrix.app-platform }}, ${{ matrix.instrumentation-type }})
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write
      issues: write
    strategy:
      fail-fast: false
      matrix:
        app-platform: [ flask ]
        instrumentation-type: [ auto, manual ]
    env:
      IMAGE_SUFFIX: sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-${{ github.sha }}
      # TODO: Does the LOGS_NAMESPACE need a `-$GITHUB_RUN_ID`? (Only if we run multiple Soak Tests at a time,
      # which we probably will not do?). If we do need it, move to steps below to access $GITHUB_RUN_ID.
      LOGS_NAMESPACE: ${{ github.repository }}/soak-tests-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}
    steps:
      - name: Configure Performance Test environment variables
        run: >-
          echo "NUM_OF_CPUS=$(nproc --all)" >> $GITHUB_ENV;
      - uses: actions/checkout@v2
      - name: Log in to the GitHub Container Registry
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Cache Docker layers
        uses: actions/cache@v2
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-
      - name: Extract metadata (tags, labels) for Docker
        id: docker-image-metadata
        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38
        with:
          images: ghcr.io/${{ github.repository }}-${{ env.IMAGE_SUFFIX }}
          tags: |
            type=schedule
            type=ref,event=branch
            type=ref,event=tag
            type=ref,event=pr
      # TODO: Instead of building, publishing, and pulling a Sample App Docker IMage in `docker-compose`. Could we
      # just build the image in `docker-compose` directly using `build/context`? We would lose the public Docker Image
      # but we would avoid polluting the published packages and anyone could build from the commit if they wanted to.
      - name: Build and Push Docker image
        uses: docker/build-push-action@v2
        with:
          push: true
          context: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}
          tags: ${{ steps.docker-image-metadata.outputs.tags }}
          labels: ${{ steps.docker-image-metadata.outputs.labels }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          role-to-assume: ${{ secrets.AWS_ASSUME_ROLE_ARN }}
          role-duration-seconds: 21600 # 6 Hours
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
      - name: Run All Docker Containers - Sample App + OTel Collector + Load Generator + Alarm Poller
        id: check-failure-during-performance-tests
        continue-on-error: true
        working-directory: .github/docker-performance-tests
        env:
          APP_IMAGE: ${{ fromJSON(steps.docker-image-metadata.outputs.json).tags[0] }}
          INSTANCE_ID: ${{ github.run_id }}-${{ github.run_number }}
          LISTEN_ADDRESS: 0.0.0.0:8080
          LOG_GROUP_NAME: otel-sdk-performance-tests
          LOGS_NAMESPACE: ${{ env.LOGS_NAMESPACE }}
          LOG_STREAM_NAME: ${{ env.IMAGE_SUFFIX }}
          PROCESS_COMMAND_LINE_DIMENSION_VALUE: ${{ env.PROCESS_COMMAND_LINE_DIMENSION_VALUE }}
          PROCESS_EXECUTABLE_NAME: ${{ env.PROCESS_EXECUTABLE_NAME }}
          HOSTMETRICS_INTERVAL_SECS: ${{ env.HOSTMETRICS_INTERVAL_SECS }}
          TEST_DURATION_MINUTES: ${{ env.TEST_DURATION_MINUTES }}
          NUM_OF_CPUS: ${{ env.NUM_OF_CPUS }}
          CPU_LOAD_THRESHOLD: ${{ env.CPU_LOAD_THRESHOLD }}
          TOTAL_MEMORY_THRESHOLD: ${{ env.TOTAL_MEMORY_THRESHOLD }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
        run: >-
          docker-compose up;
          exit $(docker inspect $(docker ps --quiet --all --filter "name=docker-performance-tests_alarms-poller") --format="{{.State.ExitCode}}");
      - name: Install script dependencies
        run: pip install boto3
      - name: Get a snapshot of metrics
        run: python3 .github/scripts/performance-tests/produce-metric-widget-images.py
          --cpu-load-threshold ${{ env.CPU_LOAD_THRESHOLD }}
          --logs-namespace ${{ env.LOGS_NAMESPACE }}
          --metrics-period ${{ env.HOSTMETRICS_INTERVAL_SECS }}
          --num-of-cpus ${{ env.NUM_OF_CPUS }}
          --process-command-line-dimension-value "${{ env.PROCESS_COMMAND_LINE_DIMENSION_VALUE }}"
          --test-duration-minutes ${{ env.TEST_DURATION_MINUTES }}
          --total-memory-threshold ${{ env.TOTAL_MEMORY_THRESHOLD }}
          --app-platform ${{ matrix.app-platform }}
          --github-sha ${{ github.sha }}
          --github-run-id $GITHUB_RUN_ID
          --instrumentation-type ${{ matrix.instrumentation-type }}
      - name: Commit snapshots to repository
        run: |-
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com";
          git config user.name "GitHub Actions";
          git checkout main;
          git pull;
          git checkout gh-pages;
          git add soak-tests/snapshots;
          git commit -m "Adding Soak Tests Snapshots from ${{ github.sha }}";
          git push;
      - name: Move back to the main branch
        run: git checkout main
      - name: Prepare Performance Test results as JSON output
        run: python3 .github/scripts/performance-tests/produce-performance-test-results.py
          --cpu-load-threshold ${{ env.CPU_LOAD_THRESHOLD }}
          --logs-namespace ${{ env.LOGS_NAMESPACE }}
          --metrics-period ${{ env.HOSTMETRICS_INTERVAL_SECS }}
          --num-of-cpus ${{ env.NUM_OF_CPUS }}
          --process-command-line-dimension-value "${{ env.PROCESS_COMMAND_LINE_DIMENSION_VALUE }}"
          --test-duration-minutes ${{ env.TEST_DURATION_MINUTES }}
          --total-memory-threshold ${{ env.TOTAL_MEMORY_THRESHOLD }}
      - name: Report on Performance Test Averages results
        uses: NathanielRN/github-action-benchmark@v1.8.2-alpha3
        continue-on-error: true
        id: check-failure-after-performance-tests
        with:
          name: Soak Test Results - sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}
          tool: custombenchmark
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          max-items-in-chart: 100
          alert-threshold: 175%
          # Does not work as expected, see: https://github.com/open-telemetry/opentelemetry-python/pull/1478
          # comment-always: true
          fail-on-alert: true
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: soak-tests/per-commit-overall-results
      - name: Publish Issue if failed DURING Performance Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-during-performance-tests.outcome == 'failure'
        env:
          APP_PLATFORM: ${{ matrix.app-platform }}
          INSTRUMENTATION_TYPE: ${{ matrix.instrumentation-type }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_SERVER_URL: ${{ env.GITHUB_SERVER_URL }}
          GITHUB_REPOSITORY: ${{ env.GITHUB_REPOSITORY }}
          GITHUB_RUN_ID: ${{ env.GITHUB_RUN_ID }}
        with:
          filename: .github/issue-templates/FAILURE-DURING-SOAK_TESTS.md
      - name: Publish Issue if failed AFTER Performance Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-performance-tests.outcome == 'failure'
        env:
          APP_PLATFORM: ${{ matrix.app-platform }}
          INSTRUMENTATION_TYPE: ${{ matrix.instrumentation-type }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_SERVER_URL: ${{ env.GITHUB_SERVER_URL }}
          GITHUB_REPOSITORY: ${{ env.GITHUB_REPOSITORY }}
          GITHUB_RUN_ID: ${{ env.GITHUB_RUN_ID }}
        with:
          filename: .github/issue-templates/FAILURE-AFTER-SOAK_TESTS.md
      - name: Check for Performance Degradation either DURING or AFTER Performance Tests
        if: steps.check-failure-during-performance-tests.outcome == 'failure' || steps.check-failure-after-performance-tests.outcome == 'failure'
        run: >-
          echo 'Performance Tests failed, see the logs above for details';
          exit 1;
