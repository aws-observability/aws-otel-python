# Pre-requisites:
# - AWS Account with correct permissions
# - `gh-pages` branch
# - AWS Account needs to add the LOGS_GROUP defined below

name: Soak Testing
on:
  schedule:
    - cron: '0 */6 * * *' # DEBUGGING: Every 6 hours
  # push:
  #   branches: [ main ]
env:
  # Language Specific
  PROCESS_COMMAND_LINE: /usr/local/bin/python3 application.py
  PROCESS_EXECUTABLE_NAME: python3
  # - Every language must also set the path for the sample-app in the `Build and Push Docker image` step.
  #   For example, Python is uses this path for it's sample-app context:
  #   `context: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}`

  # Global Env Vars
  AWS_REGION: us-west-2
  TEST_DURATION_MINUTES: 300
  HOSTMETRICS_INTERVAL_SECS: 600
  CPU_LOAD_THRESHOLD: 75
  TOTAL_MEMORY_THRESHOLD: 1610612736
  # TODO: We might be able to adapt the "Soak Tests" to be "Overhead Tests". This means monitoring the Sample App's
  # performance using high levels of TPS for the Load Generator over a shorter period of testing time.
  # For example: https://github.com/aws-observability/aws-otel-collector/blob/main/docs/performance_model.md
  # THROUGHPUT_PER_SECOND: TBD?

jobs:
  test_apps_and_publish_results:
    name: Publish app and Soak Test - (${{ matrix.app-platform }}, ${{ matrix.instrumentation-type }})
    runs-on: ubuntu-latest
    permissions:
      contents: write
      packages: write
      issues: write
    strategy:
      fail-fast: false
      matrix:
        app-platform: [ flask ]
        # instrumentation-type: [ auto, manual ]
        instrumentation-type: [ auto ]
    env:
      IMAGE_SUFFIX: sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-${{ github.sha }}
    steps:
      - name: Configure Soak Test environment variables
        # TODO: Does the LOGS_NAMESPACE need a `-$GITHUB_RUN_ID`? (Only if we run multiple Soak Tests at a time,
        # which we probably will not do?).
        run: >-
          echo "LOGS_NAMESPACE=${{ github.repository }}/soak-tests" >> $GITHUB_ENV;
          echo "NUM_OF_CPUS=$(nproc --all)" >> $GITHUB_ENV;
      - uses: actions/checkout@v2
      - name: Log in to the GitHub Container Registry
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Cache Docker layers
        uses: actions/cache@v2
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-
      - name: Extract metadata (tags, labels) for Docker
        id: docker-image-metadata
        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38
        with:
          images: ghcr.io/${{ github.repository }}-${{ env.IMAGE_SUFFIX }}
          tags: |
            type=schedule
            type=ref,event=branch
            type=ref,event=tag
            type=ref,event=pr
      # TODO: Instead of building, publishing, and pulling a Sample App Docker IMage in `docker-compose`. Could we
      # just build the image in `docker-compose` directly using `build/context`? We would lose the public Docker Image
      # but we would avoid polluting the published packages and anyone could build from the commit if they wanted to.
      - name: Build and Push Docker image
        uses: docker/build-push-action@v2
        with:
          push: true
          context: integration-test-apps/${{ matrix.instrumentation-type}}-instrumentation/${{ matrix.app-platform }}
          tags: ${{ steps.docker-image-metadata.outputs.tags }}
          labels: ${{ steps.docker-image-metadata.outputs.labels }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Prepare Metric Data Queries JSON Inputs
        run: >-
          for metric_kind in cpu-load total-memory;
          do
            echo "$(
              cat ./.github/aws-cloudwatch-api-values/get-metric-data/$metric_kind-metric-data-query.json |
              sed -e "s/<METRIC_PERIOD>/${{ env.HOSTMETRICS_INTERVAL_SECS }}/g" \
                  -e "s/<NUM_OF_CPUS>/$NUM_OF_CPUS/g" \
                  -e "s/<LOGS_NAMESPACE>/$(echo ${{ env.LOGS_NAMESPACE }} | sed 's/\//\\\//g')/g" \
                  -e "s/<PROCESS_COMMAND_LINE_DIMENSION_VALUE>/$(echo ${{ env.PROCESS_COMMAND_LINE }} | sed 's/\//\\\//g')/g"
            )" |
            tee ./.github/aws-cloudwatch-api-values/get-metric-data/prepared-$metric_kind-metric-data-query.json;
          done
      - name: Create Soak Test alarms
        run: >-
          declare -a COMMON_API_PARAMETERS=(
          --evaluation-periods 5
          --datapoints-to-alarm 3
          --comparison-operator GreaterThanOrEqualToThreshold
          --treat-missing-data ignore
          );
          aws cloudwatch put-metric-alarm "${COMMON_API_PARAMETERS[@]}" \
            --alarm-name 'OTel Python Soak Tests - CPU Load Percentage Spike' \
            --alarm-description 'Triggers when the CPU Load Percentage spikes above the allowed threshold DURING the Soak Test.' \
            --threshold ${{ env.CPU_LOAD_THRESHOLD }} \
            --cli-input-json "{ \"Metrics\": $(cat ./.github/aws-cloudwatch-api-values/get-metric-data/prepared-cpu-load-metric-data-query.json) }"
          aws cloudwatch put-metric-alarm "${COMMON_API_PARAMETERS[@]}" \
            --alarm-name 'OTel Python Soak Tests - Virtual Memory Usage Spike' \
            --alarm-description 'Triggers when the Virtual Memory Usage spikes above the allowed threshold DURING the Soak Test.' \
            --threshold ${{ env.TOTAL_MEMORY_THRESHOLD }} \
            --cli-input-json "{ \"Metrics\": $(cat ./.github/aws-cloudwatch-api-values/get-metric-data/prepared-total-memory-metric-data-query.json) }"
      - name: Run All Docker Containers - Sample App + OTel Collector + Load Generator + Alarm Poller
        id: check-failure-during-soak-tests
        continue-on-error: true
        working-directory: .github/docker-soak-tests
        env:
          APP_IMAGE: ${{ fromJSON(steps.docker-image-metadata.outputs.json).tags[0] }}
          INSTANCE_ID: ${{ github.run_id }}-${{ github.run_number }}
          LISTEN_ADDRESS: 0.0.0.0:8080
          LOG_GROUP_NAME: otel-soak-tests
          LOGS_NAMESPACE: ${{ env.LOGS_NAMESPACE }}
          LOG_STREAM_NAME: ${{ env.IMAGE_SUFFIX }}
          PROCESS_COMMAND_LINE: ${{ env.PROCESS_COMMAND_LINE }}
          PROCESS_EXECUTABLE_NAME: ${{ env.PROCESS_EXECUTABLE_NAME }}
          HOSTMETRICS_INTERVAL_SECS: ${{ env.HOSTMETRICS_INTERVAL_SECS }}
          TEST_DURATION_MINUTES: ${{ env.TEST_DURATION_MINUTES }}
        run: >-
          docker-compose up;
          exit $(docker inspect $(docker ps --quiet --all --filter "name=docker-soak-tests_alarms-poller") --format="{{.State.ExitCode}}");
      - name: Get a snapshot of metrics
        run: >-
          mkdir -p soak-tests/snapshots/${{ github.sha }};
          for metric_kind in cpu-load total-memory;
          do
            aws cloudwatch get-metric-widget-image --metric-widget "$(
              cat ./.github/aws-cloudwatch-api-values/get-metric-widget-statistics/$metric_kind-metric-widget.json |
              sed -e "s/<TEST_DURATION_MINUTES>/${{ env.TEST_DURATION_MINUTES }}/g" \
                  -e "s/<METRIC_PERIOD>/${{ env.HOSTMETRICS_INTERVAL_SECS }}/g" \
                  -e "s/<CPU_LOAD_THRESHOLD>/${{ env.CPU_LOAD_THRESHOLD }}/g" \
                  -e "s/<TOTAL_MEMORY_THRESHOLD>/${{ env.TOTAL_MEMORY_THRESHOLD }}/g" \
                  -e "s/<NUM_OF_CPUS>/$NUM_OF_CPUS/g" \
                  -e "s/<LOGS_NAMESPACE>/$(echo ${{ env.LOGS_NAMESPACE }} | sed 's/\//\\\//g')/g" \
                  -e "s/<PROCESS_COMMAND_LINE_DIMENSION_VALUE>/$(echo ${{ env.PROCESS_COMMAND_LINE }} | sed 's/\//\\\//g')/g"
            )" |
            jq -r '.MetricWidgetImage' |
            base64 -d > soak-tests/snapshots/${{ github.sha }}/${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}-$metric_kind-results-$GITHUB_RUN_ID.png;
          done
      - name: Commit snapshots to repository
        uses: EndBug/add-and-commit@v7
        with:
          add: soak-tests/snapshots
          branch: gh-pages
          message: 'Adding Soak Tests Snapshots'
      - name: Move back to the main branch
        run: git checkout main
      - name: Prepare Soak Test results as JSON output
        run: >-
          declare -a COMMON_API_PARAMETERS=(
          --start-time $(date -u -d '${{ env.TEST_DURATION_MINUTES }} minutes ago' +%FT%TZ)
          --end-time $(date -u +%FT%TZ)
          );
          METRIC_DATA_QUERY_INPUT_JSON=$(
            jq -n '[ inputs ] | add' \
            ./.github/aws-cloudwatch-api-values/get-metric-data/prepared-cpu-load-metric-data-query.json \
            ./.github/aws-cloudwatch-api-values/get-metric-data/prepared-total-memory-metric-data-query.json
          );
          aws cloudwatch get-metric-data "${COMMON_API_PARAMETERS[@]}" --cli-input-json "{ \"MetricDataQueries\": $METRIC_DATA_QUERY_INPUT_JSON }" |
          jq "{
            benchmarks: [
              {
                Name: \"Soak Test Average CPU Load\",
                Value: first(.MetricDataResults[] | select(.Id == \"cpu_load_expr\")) | .Values | ( add / length / ${{ env.HOSTMETRICS_INTERVAL_SECS }} / $NUM_OF_CPUS * 100 ),
                Unit: \"Percent\"
              },
              {
                Name: \"Soak Test Average Virtual Memory Used\",
                Value: first( .MetricDataResults[] | select(.Id == \"total_memory_expr\") ) | .Values | ( add / length / $(echo '2^20' | bc) ),
                Unit: \"Megabytes\"
              }
            ]
          }" |
          tee output.json
      - name: Report on Soak Test Averages results
        uses: NathanielRN/github-action-benchmark@v1.8.2-alpha3
        continue-on-error: true
        id: check-failure-after-soak-tests
        with:
          name: Soak Test Results - sample-app-${{ matrix.app-platform }}-${{ matrix.instrumentation-type }}
          tool: custombenchmark
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          max-items-in-chart: 100
          alert-threshold: 175%
          # Does not work as expected, see: https://github.com/open-telemetry/opentelemetry-python/pull/1478
          # comment-always: true
          fail-on-alert: true
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: soak-tests/per-commit-overall-results
      - name: Publish Issue if failed DURING Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-during-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_SERVER_URL: ${{ env.GITHUB_SERVER_URL }}
          GITHUB_REPOSITORY: ${{ env.GITHUB_REPOSITORY }}
          GITHUB_RUN_ID: ${{ env.GITHUB_RUN_ID }}
        with:
          filename: .github/issue-templates/FAILURE-DURING-SOAK_TESTS.md
      - name: Publish Issue if failed AFTER Soak Tests
        uses: JasonEtco/create-an-issue@v2
        if: steps.check-failure-after-soak-tests.outcome == 'failure'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_SERVER_URL: ${{ env.GITHUB_SERVER_URL }}
          GITHUB_REPOSITORY: ${{ env.GITHUB_REPOSITORY }}
          GITHUB_RUN_ID: ${{ env.GITHUB_RUN_ID }}
        with:
          filename: .github/issue-templates/FAILURE-AFTER-SOAK_TESTS.md
      - name: Delete Soak Test alarms
        run: >-
          aws cloudwatch delete-alarms --alarm-names 'OTel Python Soak Tests - CPU Load Percentage Spike' 'OTel Python Soak Tests - Virtual Memory Usage Spike'
      - name: Check for Performance Degradation either DURING or AFTER Soak Tests
        if: steps.check-failure-after-soak-tests.outcome == 'failure' || steps.check-failure-after-soak-tests.outcome == 'failure'
        run: >-
          echo 'Soak Tests failed, see the logs above for details';
          exit 1;
